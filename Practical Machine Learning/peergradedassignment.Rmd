---
title: "Peer-graded Assignment"
author: "Brynjólfur Gauti Jónsson"
date: '`r format(Sys.Date())`'
output:
  md_document:
    variant: markdown_github
  html_document: default
  html_notebook:
    toc: yes
  pdf_document:
    toc: yes
---
```{r, message=FALSE}
library(ggplot2); library(ggthemes); library(caret); library(parallel) 
library(doParallel); library(randomForest)
```

# Synopsis

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. The purpose of this analysis is to utilize data from accelerometers such as the above fastened on the belt, forearm, arm and dumbbell of participants to predict how they performed certain exercises. Six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Those 5 ways will be the output classes of our model.

*[More information is available here]( http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har)*

# Getting and Cleaning the Data

## Reading the data
```{r datareading, cache=TRUE}
set.seed(101)
training <- read.csv('training.csv')
testing <- read.csv('testing.csv')
dim(training)
```

In the data there are lots of NA’s and lots of reduntant features. Next we will remove unneeded features and features with a high number of NA's.

## Cleaning the Data

There are a lot more variables than we need. We will filter out all variables pertaining to means, standard deviations, kurtosis etc. since they are combinations of all other variables. To do this I have written a function called removeFeatures.

```{r removefeatures, cache=TRUE}
removeFeatures <- function(xtrain) {
    xtrain <- xtrain[,-grep('[Kk]urtosis',names(xtrain))]
    xtrain <- xtrain[,-grep('[Ss]kewness',names(xtrain))]
    xtrain <- xtrain[,-grep('[Ss]tddev',names(xtrain))]
    xtrain <- xtrain[,-grep('[Aa]vg',names(xtrain))]
    xtrain <- xtrain[,-grep('[Vv]ar',names(xtrain))]
    xtrain <- xtrain[,-grep('[Mm]in',names(xtrain))]
    xtrain <- xtrain[,-grep('[Mm]ax',names(xtrain))]
    xtrain <- xtrain[,-c(1,2)] # Remove the name of participant and obs. number.
    xtrain <- xtrain[,-c(12, 43, 59)] # These variables were all na in the test set.
    xtrain <- xtrain[,colMeans(is.na(xtrain)) < 0.9]
    return(xtrain)
}
train <- removeFeatures(training)
xtrain <- train[,-ncol(train)]
ytrain <- train[,ncol(train)]
test <- testing[,names(testing) %in% names(train)] 
```

\newpage

# Modelling

## Model Settings and PreProcessing

Now we prepare the data for modelling. There is a lot of missing data so we will utilize knnImpute to make new data points where needed. Since there are so many variables and no need for interpretability of the model, we will center and scale the data, and reduce the dimensions of the variable space via Principal Components Analysis. This will decrease the amount of space needed to store the data and shorten training time.

```{r modelparams, cache=TRUE}
library(caret)
preObj <- preProcess(x = xtrain, method=(c('knnImpute','center','scale', 'pca')))
xtrainfit <- predict(object = preObj, xtrain)


# Settings for the model. We use 15-fold Cross-Validation
fitSettings <- trainControl(method = 'cv', number = 5,
                            allowParallel = TRUE)


preObj
```

We are able to explain 95% of the variance in the variables using only 26 principal components. This will help our model, since random trees are susceptible to redundant features. Now we have centered, scaled and reduced data ready for modelling.

## Creating a smaller model


Since we don’t have access to the real values of the test set, we first train a model on a subset of our training set and rate it using a test set.
```{r smallmodelsettings}
inTrain <- createDataPartition(train[,ncol(train)], p = 4/5, list=FALSE)

# Training set
smalltrain <- train[inTrain,]
smalltrainx <- predict(preObj, smalltrain[,-ncol(smalltrain)])
smalltrainy <- smalltrain[,ncol(smalltrain)]
# Testing set
smalltest <- train[-inTrain,]
smalltestx <- predict(preObj,smalltest[,-ncol(smalltest)]) 
smalltesty <- smalltest[,ncol(smalltest)]
```

\newpage

### Training the smaller model

Now we are ready to train the smaller model.
```{r smallmodeltrain, cache=TRUE}
#cluster <- makeCluster(detectCores() - 1)
#registerDoParallel(cluster)
#smallmodel <- train(x = smalltrainx[,-1], y = smalltrainy, method = 'rf',
#                    trControl = fitSettings)
#saveRDS(smallmodel, 'smallrfModel.rds')
#stopCluster(cluster)
#registerDoSEQ()
smallmodel <- readRDS('smallrfModel.rds')
```

## Testing the smaller model

Now we see how well the smaller model works on the test holdout set.

```{r}
smallpred <- predict(smallmodel, smalltestx) 
confusionMatrix(smallpred, smalltesty)
```

\newpage

```{r confusionPlot, cache=TRUE}
g <- ggplot(data = data.frame(pred = smallpred, True=smalltesty,
misclas = (smallpred==smalltesty)))

g + geom_jitter(aes(x = pred, y = True, col=misclas)) + theme_tufte() +
    xlab('Predicted Category') + ylab('True Category') +
ggtitle('Confusion Matrix Plot') + labs(color='Prediction Is Correct') +
    scale_colour_manual(values = c('mediumorchid', 'black'))
```

There seems to be no pattern in our prediction errors. We are now ready to train a full-size model and use it to predict on our original test set and answer all questions on the quiz.


\newpage

## Training the Full Model

We utilize the parallel and doParallel packages to quicken the training process. The model has previously been saved into the file ‘rfMOdel.rds’ and is loaded from there instead of training, but the code for the model is availiable below.

This model has previously been trained and saved into a file, but the code follows for reproducibility.

```{r modeltraining, cache=TRUE}
#library(parallel)
#library(doParallel)
#cluster <- makeCluster(detectCores() - 1)
#registerDoParallel(cluster)
#model <- train(x = xtrain, y = ytrain, method = 'rf',
#               trControl = fitSettings)
#saveRDS(model, 'rfModel.rds')
#stopCluster(cluster)
#registerDoSEQ()

model <- readRDS('rfModel.rds')
```




## Testing the Full Model

Now we apply the same pre-processing to the testing data, as we did to the training data, after which we find our predictions.
```{r, cache=TRUE}

model

xtest <- predict(preObj, test)
pred <- predict(model, xtest)
pred

```

# Summary

In this assignment I read and cleaned data meassured by motion sensors mounted on various parts of participants' bodies. This data was then used to predict the manner in which they did certain exercises. After cleaning the data and preprocessing it by centering, scaling, and reducing its dimensions, random forests was the preferred model. There was little need for interpretability of the model and no need for major scalability, and RF is an extremely accurate algorithm that sacrifices both of those perks.


